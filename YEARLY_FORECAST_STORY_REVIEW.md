# Yearly Forecast Story Pipeline - Code Review & Fixes

## Overview

This document summarizes the review and fixes applied to the new `/v1/forecasts/yearly/forecast` endpoint implementation, which generates LLM-interpreted yearly forecast reports with PDF rendering.

## Architecture Summary

### Endpoint Flow
```
POST /v1/forecasts/yearly/forecast
  ‚Üì
1. yearly_forecast_report.py (router)
   ‚îî‚îÄ‚îÄ generate_yearly_forecast_with_pdf()
       ‚Üì
2. build_interpreted_yearly_report()
   ‚îú‚îÄ‚îÄ compute_yearly_forecast() ‚Üí calls existing yearly_payload()
   ‚îî‚îÄ‚îÄ interpret_yearly_forecast() ‚Üí LLM interpretation
       ‚Üì
3. render_yearly_report_pdf()
   ‚îî‚îÄ‚îÄ render_western_natal_pdf() with yearly_v2.html.j2 template
       ‚Üì
4. Returns: YearlyForecastReportResponse
   ‚îú‚îÄ‚îÄ report: YearlyForecastReport (structured JSON)
   ‚îî‚îÄ‚îÄ pdf_download_url: string
```

### Key Components

#### 1. **Router** (`api/routers/yearly_forecast_report.py`)
- Exposes `POST /v1/forecasts/yearly/forecast`
- Handles `YearlyForecastRequest` (reuses existing schema from `/v1/forecasts/yearly`)
- Returns `YearlyForecastReportResponse` with structured report + PDF URL
- Properly registered in `api/app.py` line 85

#### 2. **Orchestration** (`api/services/yearly_forecast_report.py`)
- `compute_yearly_forecast()`: Calls existing `yearly_payload()` asynchronously
- `build_interpreted_yearly_report()`: Orchestrates raw computation + LLM interpretation
- `render_yearly_report_pdf()`: Renders PDF using yearly_v2 template, handles S3 upload
- `generate_yearly_forecast_with_pdf()`: Main entry point combining all steps

#### 3. **LLM Client** (`api/services/llm_client.py`)
- Wrapper around OpenAI `AsyncOpenAI` client
- Configurable via `OPENAI_API_KEY` and `OPENAI_MODEL` environment variables
- Graceful fallback if OpenAI package not installed
- Raises `LLMUnavailableError` if configuration is missing

#### 4. **Interpretation Pipeline** (`api/services/yearly_interpreter.py`)
- **Rule-based classification**: `classify_event()` assigns themes (career, relationships, health, spiritual, innovation)
- **LLM-driven narratives**: Generates text for:
  - Year overview commentary
  - Monthly overviews
  - Theme-specific sections (career, relationships, health)
  - Eclipse/lunation guidance
  - Rituals and journal prompts
  - Planner action items
- **Heatmap generation**: Calculates monthly intensity and peak scores
- **Fallback handling**: If LLM unavailable, uses generic motivational text

#### 5. **PDF Rendering** (`api/services/pdf_renderer.py`)
- **WeasyPrint path** (if `USE_WEASY=true`): Uses `yearly_v2.html.j2` Jinja2 template
- **ReportLab fallback**: `_render_story_yearly()` creates structured PDF programmatically
- Dispatches correctly based on payload structure (checks for `"report"` key)

#### 6. **Template** (`api/templates/reports/yearly_v2.html.j2`)
- Accesses context via `data.report.*` and `data.generated_at`
- Sections: Cover, Year at a Glance, Eclipses, Monthly narratives, Appendices
- Properly structured for 150-page outline format

#### 7. **Schemas** (`api/schemas/yearly_forecast_report.py`)
- `YearlyForecastRequest`: Alias of base `YearlyForecastRequest` from forecasts
- `YearlyForecastReport`: Main report structure
  - `year_at_glance`: Heatmap, top events, commentary
  - `eclipses_and_lunations`: List of eclipse summaries
  - `months`: Monthly sections with narratives, high/caution days, planner actions
  - `appendix_all_events`: Flat event list
  - `glossary`: Term definitions
  - `interpretation_index`: Event summaries
- `YearlyForecastReportResponse`: Wrapper with report + pdf_download_url

---

## Issues Found & Fixed

### ‚úÖ Issue #1: Invalid OpenAI Model Name (CRITICAL)
**File**: `api/services/llm_client.py` line 35  
**Problem**: Model name was `"gpt-4.1-mini"` which doesn't exist  
**Fix**: Changed to `"gpt-4o-mini"` (valid OpenAI model)  
**Impact**: Would cause LLM API calls to fail with model not found error

```python
# Before
model_name = model or os.getenv("OPENAI_MODEL", "gpt-4.1-mini")

# After
model_name = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
```

---

### ‚úÖ Issue #2: Pydantic v1 Syntax (BUG)
**File**: `api/services/yearly_interpreter.py` line 282  
**Problem**: Used `.copy(update={...})` which is Pydantic v1 syntax  
**Fix**: Changed to `.model_copy(update={...})` for Pydantic v2 compatibility  
**Impact**: Would cause AttributeError at runtime when updating eclipse guidance

```python
# Before
eclipses = [e.copy(update={"guidance": eclipse_text}) for e in eclipses]

# After
eclipses = [e.model_copy(update={"guidance": eclipse_text}) for e in eclipses]
```

---

### ‚úÖ Issue #3: Wrong isinstance Check (MINOR)
**File**: `api/services/yearly_forecast_report.py` line 96  
**Problem**: Checked `isinstance(outputs, dict)` but accessed `options.get("base_url")`  
**Fix**: Changed to `isinstance(options, dict)` for consistency  
**Impact**: Logic worked by accident (outputs is always dict), but semantically incorrect

```python
# Before
base_url = options.get("base_url") if isinstance(outputs, dict) else None

# After
base_url = options.get("base_url") if isinstance(options, dict) else None
```

---

## Architecture Validation

### ‚úÖ Does NOT Modify Existing `/v1/forecasts/yearly` Behavior
- Uses existing `yearly_payload()` via `forecast_builders.py`
- No changes to core astro calculation code in `yearly_western.py`
- Router is separate (`yearly_forecast_report.py` vs `forecasts.py`)

### ‚úÖ LLM Integration (OpenAI)
- **Model**: GPT-4o-mini (configurable via `OPENAI_MODEL` env var)
- **Temperature**: 0.7
- **Max tokens**: 800 per section (300 for bullets)
- **System prompt**: Empathetic, practical, non-fatalistic tone
- **Async**: Uses `AsyncOpenAI` for non-blocking calls
- **Parallel calls**: Uses `asyncio.gather()` to generate multiple sections simultaneously
- **Fallback**: Returns generic text if LLM unavailable

### ‚úÖ Rule-Based Bucketing
- **Theme classification**: Based on planet/body names and note keywords
- **High score days**: Top 8 events sorted by score
- **Caution days**: Events with challenging aspects (square, opposition, quincunx)
- **Section assignment**: Each event assigned to primary theme (career, relationships, health, etc.)

### ‚úÖ Structured Report Model (Mirrors 150-Page Outline)
- **Front matter**: Cover page with year, profile, generation timestamp
- **Year at a Glance**: Heatmap, top events, commentary
- **Eclipses & Lunations**: Guidance for major lunar events
- **Monthly sections** (12):
  - Overview
  - Career & Finance
  - Relationships & Family
  - Health & Energy
  - Rituals & Journal prompts
  - Planner actions
  - High score days
  - Caution days
  - Aspect grid
- **Appendices**:
  - A: All events (chronological)
  - B: Glossary
  - C: Interpretation Index

### ‚úÖ PDF Rendering
- **Template**: `yearly_v2.html.j2` with proper context access
- **WeasyPrint**: Full HTML/CSS rendering when enabled
- **ReportLab fallback**: `_render_story_yearly()` creates structured PDF programmatically
- **Storage options**: Local filesystem or S3 upload
- **URL resolution**: Supports custom base URLs or S3 public URLs

### ‚úÖ API Integration
- Router registered in `api/app.py` line 85
- Proper exception handling for `LLMUnavailableError`
- Async/await throughout the pipeline

---

## Testing Recommendations

### 1. Unit Tests Needed
- `test_yearly_interpreter.py`:
  - `classify_event()` with various bodies and notes
  - Heatmap generation
  - Eclipse extraction
  - LLM fallback behavior
- `test_llm_client.py`:
  - Mock OpenAI responses
  - Test error handling when API key missing
  - Test model override via env var
- `test_yearly_forecast_report.py`:
  - End-to-end pipeline with mocked LLM
  - S3 upload logic
  - URL resolution

### 2. Integration Tests
- Full request to `/v1/forecasts/yearly/forecast` with valid OpenAI key
- Verify PDF generation (both WeasyPrint and ReportLab paths)
- Test with missing `OPENAI_API_KEY` (should fail gracefully)
- Test with `include_interpretation=false` (if supported)

### 3. Manual Testing
```bash
# Set OpenAI key
export OPENAI_API_KEY="sk-..."

# Start server
python -m uvicorn api.app:app --reload --port 8081

# Test request
curl -X POST http://localhost:8081/v1/forecasts/yearly/forecast \
  -H "Content-Type: application/json" \
  -d '{
    "chart_input": {
      "system": "western",
      "date": "1990-01-01",
      "time": "12:00:00",
      "time_known": true,
      "place": {"lat": 40.7128, "lon": -74.0060, "tz": "America/New_York"},
      "options": {"zodiac": "tropical", "house_system": "placidus"}
    },
    "options": {
      "year": 2025,
      "step_days": 1,
      "transit_bodies": ["Sun", "Mercury", "Venus", "Mars", "Jupiter", "Saturn"],
      "aspects": {
        "types": ["conjunction", "opposition", "square", "trine", "sextile"],
        "orb_deg": 3.0
      }
    }
  }'
```

### 4. Performance Testing
- Measure total response time (computation + LLM + PDF)
- Expected: 20-60 seconds depending on LLM latency
- Monitor OpenAI token usage per request
- Test GPU acceleration impact on computation phase

---

## Configuration

### Required Environment Variables
```bash
# OpenAI API key (required for LLM interpretation)
OPENAI_API_KEY=sk-proj-...

# Optional: Override default model
OPENAI_MODEL=gpt-4o-mini

# Optional: Enable WeasyPrint for better PDF rendering
USE_WEASY=true
```

### Optional Configuration
- `outputs.s3.bucket`: S3 bucket name for storage
- `outputs.s3.prefix`: S3 key prefix
- `outputs.s3.public`: Make PDF publicly accessible
- `options.base_url`: Custom base URL for download links

---

## Code Quality Assessment

### ‚úÖ Strengths
1. **Clean separation of concerns**: Router ‚Üí Orchestration ‚Üí Interpretation ‚Üí Rendering
2. **Proper async/await**: Non-blocking LLM calls with `asyncio.gather()`
3. **Graceful fallbacks**: Works without LLM (generic text) and without WeasyPrint (ReportLab)
4. **Reusability**: Reuses existing `yearly_payload()` without modification
5. **Type safety**: Pydantic schemas for request/response validation
6. **Comprehensive structure**: Mirrors 150-page outline format
7. **Proper error handling**: `LLMUnavailableError` raised and caught at router level

### ‚ö†Ô∏è Potential Improvements
1. **LLM token usage tracking**: Add logging for token consumption per request
2. **Caching**: Consider caching LLM responses for identical prompts (development)
3. **Prompt versioning**: Track prompt templates for reproducibility
4. **Timeout handling**: Add timeouts for LLM calls (currently relies on OpenAI client defaults)
5. **Cost monitoring**: Log estimated API costs per report generation
6. **A/B testing**: Support multiple system prompts or interpretation styles
7. **Validation**: Add schema validation for LLM responses (optional structured outputs)

### üìã Documentation Needs
1. API endpoint documentation (OpenAPI/Swagger)
2. Example requests/responses
3. LLM prompt engineering guide
4. Cost estimation guide (tokens per report)
5. S3 configuration guide

---

## Deployment Considerations

### Development
- Set `OPENAI_API_KEY` in `.env` file
- Use default `gpt-4o-mini` model (cost-effective)
- Enable `USE_WEASY=true` for better PDF quality

### Production
- Secure `OPENAI_API_KEY` via secrets management (AWS Secrets Manager, etc.)
- Consider rate limiting on endpoint (expensive LLM calls)
- Monitor OpenAI API quota and usage
- Set up S3 bucket for PDF storage
- Enable CloudWatch logging for LLM errors
- Consider using `gpt-3.5-turbo` if cost is critical

### Docker
- Add `openai` package to requirements
- Ensure Cairo/Pango dependencies for WeasyPrint (if enabled)
- Mount `.env` or pass environment variables

---

## Summary

The `/v1/forecasts/yearly/forecast` implementation is **well-architected and production-ready** after applying the 3 bug fixes:

1. ‚úÖ Fixed invalid OpenAI model name
2. ‚úÖ Updated to Pydantic v2 syntax
3. ‚úÖ Corrected isinstance check

### Key Features
- ‚úÖ Calls existing yearly engine without modification
- ‚úÖ LLM-driven narrative interpretation (OpenAI GPT-4o-mini)
- ‚úÖ Rule-based theme classification
- ‚úÖ Structured 150-page report format
- ‚úÖ PDF rendering (WeasyPrint or ReportLab)
- ‚úÖ S3 upload support
- ‚úÖ Proper async/await throughout
- ‚úÖ Graceful fallbacks

### Next Steps
1. Add comprehensive test suite
2. Set `OPENAI_API_KEY` in environment
3. Test with sample request
4. Monitor LLM token usage and costs
5. Add API documentation

---

**Reviewed by**: AI Assistant  
**Date**: 2025-11-19  
**Status**: ‚úÖ APPROVED (with fixes applied)

